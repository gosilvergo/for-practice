<!DOCTYPE HTML>
<html>
<head>
    <title>What is GPT</title>
    <meta charset="utf-8">
</head>
</html>
<body>
    <h1><a href="What is GPT.html">What is GPT</a></h1>
    <ol>
        <li><a href="1.html">History</a></li>
        <li><a href="2.html">Foundational models</a></li>
        <li><a href="3.html">Task-specific models</a></li>
    </ol>
    <h2>Foundational models</h2>
    <p>A <a title="A foundation model is a large machine learning (ML) model trained on a vast quantity of data at scale such that it can be adapted to a wide range of downstream tasks.">foundational model</a> is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.<a title="'Introducing the Center for Research on Foundation Models (CRFM)'. Stanford HAI.">[25]</a></p>
    <p>Thus far, the most notable GPT foundation models have been from <a title="OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc.">OpenAI</a>'s GPT-n series. The most recent from that is GPT-4, for which OpenAI declined to publish the size or training details (citing "the competitive landscape and the safety implications of large-scale models").<a title="OpenAI (2023). 'GPT-4 Technical Report' (PDF). Archived (PDF) from the original on 2023-03-14. Retrieved 2023-03-16." href="https://cdn.openai.com/papers/gpt-4.pdf" target="_blank">[26]</a></p>
    <p>Other such models include Google's <a title="PaLM is a 540 billion parameter transformer-based large language model developed by Google AI. Researchers also trained smaller versions of PaLM, 8 and 62 billion parameter models, to test the effects of model scale.">PaLM</a>, a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API,<a title="Vincent, James (March 14, 2023). 'Google opens up its AI language model PaLM to challenge OpenAI and GPT-3'. The Verge." href="https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai" target="_blank">[33]</a><a title="'Google Opens Access to PaLM Language Model'.">[34]</a> and Together's <strong>GPT-JT</strong>, which has been reported as the closest-performing <a href="https://en.wikipedia.org/wiki/Open_source" target="_blank">open-source</a> alternative to GPT-3 (and is derived from earlier open-source GPTs).<a title="Iyer, Aparna (November 30, 2022). 'Meet GPT-JT, the Closest Open Source Alternative to GPT-3'. Analytics India Magazine.">[35]</a> <a title="Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. Meta AI intends to develop various forms of artificial intelligence, improving augmented and artificial reality technologies. Meta AI is an academic research laboratory focused on generating knowledge for the AI community.">Meta AI</a> (formerly <a title="Facebook is an online social media and social networking service owned by American technology giant Meta Platforms.">Facebook</a>) also has a generative transformer-based foundational large language model, known as <a title="LLaMA is a family of large language models (LLMs), released by Meta AI starting in February 2023.">LLaMA</a>.<a title="'Meta Debuts AI Language Model, But It's Only for Researchers'. PCMAG." href="https://www.pcmag.com/news/meta-debuts-ai-language-model-but-its-only-for-researchers" target="_blank">[36]</a></p>
    <p>Foundational GPTs can also employ <a title="In the context of human–computer interaction, a modality is the classification of a single independent channel of input/output between a computer and a human. Such channels may differ based on sensory nature, or other significant differences in processing .">modalities</a> other than text, for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text).<a title="Islam, Arham (March 27, 2023). 'Multimodal Language Models: The Future of Artificial Intelligence (AI)'">[37]</a> Regarding multimodal output, some generative transformer-based models are used for <a title="A text-to-image model is a machine learning model which takes an input natural language description and produces an image matching that description.">text-to-image</a> technologies such as <a title="In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of generative models. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset.">diffusion</a><a title="Islam, Arham (November 14, 2022). 'How Do DALL·E 2, Stable Diffusion, and Midjourney Work?'" href="https://www.marktechpost.com/2022/11/14/how-do-dall%c2%b7e-2-stable-diffusion-and-midjourney-work/" target="_blank">[38]</a> and parallel decoding.<a title="Saha, Shritama (January 4, 2023). 'Google Launches Muse, A New Text-to-Image Transformer Model'. Analytics India Magazine." href="https://analyticsindiamag.com/google-launches-muse-a-new-text-to-image-transformer-model/" target="_blank">[39]</a> Such kinds of models can serve as <strong>visual foundation models</strong> (VFMs) for developing downstream systems that can work with images.<a title="Wu (et-al), Chenfei (March 8, 2023). 'Visual ChatGPT'.">[40]</a></p>
</body>