<!DOCTYPE HTML>
<html>
<head>
    <title>History</title>
    <meta charset="utf-8">
</head>
</html>
<body>
    <h1><a href="What is GPT.html">What is GPT</a></h1>
    <ol>
        <li><a href="1.html">History</a></li>
        <li><a href="2.html">Foundational models</a></li>
        <li><a href="3.html">Task-specific models</a></li>
    </ol>
    <h2>History</h2>
    <h3>Initial developments</h3>
    <p>Generative pretraining (GP) was a long-established concept in machine learning applications,<a title="Hinton (et-al), Geoffrey (October 15, 2012). 'Deep neural networks for acoustic modeling in speech recognition' (PDF). IEEE Signal Processing Magazine. Digital Object Identifier 10.1109/MSP.2012.2205597." href="https://cs224d.stanford.edu/papers/maas_paper.pdf" target="_blank">[14]</a><a title="Deng, Li (2014-01-22). 'A tutorial survey of architectures, algorithms, and applications for deep learning | APSIPA Transactions on Signal and Information Processing | Cambridge Core.' Apsipa Transactions on Signal and Information Processing. Cambridge.org. 3: e2." href="https://www.nowpublishers.com/article/Details/SIP-014" target="_blank">[15]</a> but the <a title="A transformer is a deep learning architecture, initially proposed in 2017, that relies on the parallel multi-head attention mechanism.">transformer</a> architecture was not available until 2017 when it was invented by employees at <a title="Google LLC is an American multinational technology company focusing on artificial intelligence, online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, and consumer electronics.">Google.</a><a title="Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (December 5, 2017). 'Attention Is All You Need'">[16]</a> That development led to the emergence of large language models such as <a title="Bidirectional Encoder Representations from Transformers (BERT) is a family of language models introduced in October 2018 by researchers at Google.">BERT</a> in 2018<a title="Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (May 24, 2019). 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'">[17]</a> which was a pre-trained transformer (PT) but not designed to be <a title="Generative artificial intelligence is artificial intelligence capable of generating text, images, or other media, using generative models.">generative</a> (BERT was an "encoder-only" model).<a title="Naik, Amit Raja (September 23, 2021). 'Google Introduces New Architecture To Reduce Cost Of Transformers'">[18]</a> Also around that time, in 2018, <a title="OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc.">OpenAI</a> published its article entitled "Improving Language Understanding by Generative Pre-Training," in which it introduced the first generative pre-trained transformer (GPT) system (<a title="Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI's large language models following Google's invention of the transformer architecture in 2017.">"GPT-1"</a>).<a title="Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya (11 June 2018). 'Improving Language Understanding by Generative Pre-Training' (PDF). OpenAI. p. 12. Archived (PDF) from the original on 26 January 2021. Retrieved 23 January 2021." href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">[19]</a></p>
    <p>Prior to transformer-based architectures, the best-performing neural NLP (<a title="Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language.">natural language processing</a>) models commonly employed <a title="Supervised learning (SL) is a paradigm in machine learning where input objects and a desired output value train a model. The training data is processed, building a function that maps new data on expected output values. An optimal scenario will allow for the algorithm to correctly determine output">supervised learning</a> from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.<a title="Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya (11 June 2018). 'Improving Language Understanding by Generative Pre-Training' (PDF). OpenAI. p. 12. Archived (PDF) from the original on 26 January 2021. Retrieved 23 January 2021." href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">[19]</a></p>
    <p>The <a title="Weak supervision, also called semi-supervised learning, is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.">semi-supervised</a> approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an <a title="Unsupervised learning is a paradigm in machine learning where, in contrast to supervised learning and semi-supervised learning, algorithms learn patterns exclusively from unlabeled data.">unsupervised generative</a> "pretraining" stage to set initial parameters using a language modeling objective, and a supervised <a title="Discriminative models, also referred to as conditional models, are a class of logistical models used for classification or regression. They distinguish decision boundaries through observed data, such as pass/fail, win/lose, alive/dead or healthy/sick.">discriminative</a> "<a title="In deep learning, fine-tuning is an approach to transfer learning in which the weights of a pre-trained model are trained on new data. Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are 'frozen.'">fine-tuning</a>" stage to adapt these parameters to a target task.<a title="Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya (11 June 2018). 'Improving Language Understanding by Generative Pre-Training' (PDF). OpenAI. p. 12. Archived (PDF) from the original on 26 January 2021. Retrieved 23 January 2021." href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">[19]</a></p>
    <h3>Later developments</h3>
    <p>Regarding more recent <a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer#Foundational_models">GPT foundation models</a>, <a title="OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc.">OpenAI</a> published its first versions of <a title="Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.">GPT-3</a> in July 2020. There were three models, with 1B, 6.7B, 175B parameters, respectively named babbage, curie, and davinci (giving initials B, C, and D).<a title="This claim needs references to reliable sources.(November 2023)" href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" target="_blank">[citation needed]</a></p>
    <p>In July 2021, OpenAI published <a title="OpenAI Codex is an artificial intelligence model developed by OpenAI. It parses natural language and generates code in response. It powers GitHub Copilot, a programming autocompletion tool for select IDEs, like Visual Studio Code and Neovim.">Codex</a>, a <a href="3.html">task-specific GPT model</a> targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from <a title="GitHub, Inc. is a platform and cloud-based service for software development and version control, allowing developers to store and manage their code.">GitHub.</a><a title="Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Ponde de Oliveira Pinto, Henrique; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy (2021-07-01). 'Evaluating Large Language Models Trained on Code'" href="https://ui.adsabs.harvard.edu/abs/2021arXiv210703374C/abstract" target="_blank">[20]</a></p>
    <p>In March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001.<a title="Ouyang, Long; Wu, Jeffrey; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie (2022-12-06). 'Training language models to follow instructions with human feedback'. Advances in Neural Information Processing Systems. 35: 27730–27744.">[21]</a>, and then started beta testing code-davinci-002.<a title="'New GPT-3 capabilities: Edit & insert'. openai.com. Retrieved 2023-06-24.">[22]</a> text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and <a title="ChatGPT is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, that enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language.">ChatGPT</a> were released in November 2022, with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors), whereas ChatGPT is further trained for conversational interaction with a human user.<a title="Fu, Yao; Peng, Hao; Khot, Tushar (2022). 'How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources'. Yao Fu's Notion.">[23]</a><a title="'Model index for researchers'. OpenAI API. Archived from the original on 23 Jun 2023. Retrieved 2023-06-23.">[24]</a></p>
    <p>OpenAI's most recent GPT foundation model, <a title="Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models.">GPT-4</a>, was released on March 14, 2023. It can be accessed directly by users via a premium version of ChatGPT, and is available to developers for incorporation into other products and services via OpenAI's <a title="An application programming interface (API) is a way for two or more computer programs to communicate with each other. It is a type of software interface, offering a service to other pieces of software." href="https://en.wikipedia.org/wiki/API" target="_blank">API</a>. Other producers of GPT foundation models include <a title="EleutherAI is a grass-roots non-profit artificial intelligence (AI) research group.">EleutherAI</a> (with a series of models starting in March 2021)<a title="Alford, Anthony (July 13, 2021). 'EleutherAI Open-Sources Six Billion Parameter GPT-3 Clone GPT-J'. InfoQ.">[10]</a> and <a title="Cerebras Systems is an American artificial intelligence company with offices in Sunnyvale and San Diego, Toronto, Tokyo and Bangalore, India.">Cerebras</a> (with seven models released in March 2023).<a title="'News' (Press release)." href="https://www.businesswire.com/news/home/20230328005366/en/Cerebras-Systems-Releases-Seven-New-GPT-Models-Trained-on-CS-2-Wafer-Scale-Systems" target="_blank">[11]</a></p>
</body>